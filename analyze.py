

import pandas as pd
import numpy as np
import jieba
import jieba.analyse
from snownlp import SnowNLP
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import re
import warnings
import os
import sys
from collections import Counter
import itertools
from wordcloud import WordCloud
import matplotlib
from typing import Dict, List, Tuple, Any, Optional

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨
CUSTOM_STOPWORDS = set([
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'ç­‰', 'ä¸º', 'å¯¹', 'ä¸­', 'ä¹Ÿ', 'æœ‰', 
    'è€Œ', 'ä½†', 'å°±', 'éƒ½', 'è¿™', 'é‚£', 'ä¸€ä¸ª', 'ä¸€äº›', 'ä¹‹', 'ä¸', 'æˆ–',
    'æ—¥', 'æœˆ', 'å¹´', 'æ—¶', 'åˆ†', 'ç§’', 'å…¬å¸', 'è¡¨ç¤º', 'è®¤ä¸º', 'æŒ‡å‡º',
    'æŠ¥é“', 'æ–°é—»', 'è®°è€…', 'æ®æ‚‰', 'äº†è§£', 'ç›¸å…³', 'è¿›è¡Œ', 'å‘å±•', 'æŠ€æœ¯'
])

class DeepSeekNewsAnalyzer:
    """DeepSeekæ–°é—»æ•°æ®åˆ†æå™¨ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, file_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            file_path: æ–°é—»æ•°æ®CSVæ–‡ä»¶è·¯å¾„
        """
        self.file_path = file_path
        self.df = None
        self.analysis_results = {}
        self.setup_jieba()
        
    def setup_jieba(self):
        """è®¾ç½®ç»“å·´åˆ†è¯"""
        # æ·»åŠ DeepSeekç›¸å…³è¯æ±‡åˆ°è¯å…¸
        jieba.add_word('DeepSeek', freq=1000, tag='nz')
        jieba.add_word('æ·±åº¦æ±‚ç´¢', freq=1000, tag='nz')
        jieba.add_word('å¤§æ¨¡å‹', freq=800, tag='n')
        jieba.add_word('å¼€æºæ¨¡å‹', freq=800, tag='n')
        jieba.add_word('AIæ¨¡å‹', freq=800, tag='n')
        jieba.add_word('äººå·¥æ™ºèƒ½', freq=800, tag='n')
        
    def extract_date_from_url(self, url: str) -> Optional[pd.Timestamp]:
        """
        ä»URLä¸­æå–æ—¥æœŸï¼ˆé’ˆå¯¹CCTVç­‰æ–°é—»ç½‘ç«™æ ¼å¼ï¼‰
        
        Args:
            url: æ–°é—»ç½‘å€
            
        Returns:
            æå–çš„æ—¥æœŸæˆ–None
        """
        if not isinstance(url, str):
            return None
        
        # åŒ¹é…å¸¸è§æ—¥æœŸæ ¼å¼
        patterns = [
            r'/(\d{4})/(\d{1,2})/(\d{1,2})/',      # /2025/01/28/
            r'/(\d{4})-(\d{1,2})-(\d{1,2})/',      # /2025-01-28/
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',     # 2025å¹´01æœˆ28æ—¥
            r'(\d{4})\.(\d{1,2})\.(\d{1,2})',      # 2025.01.28
            r'(\d{4})(\d{2})(\d{2})',              # 20250128
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                try:
                    groups = match.groups()
                    if len(groups) >= 3:
                        year, month, day = groups[:3]
                        # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„æ—¥æœŸ
                        year_int, month_int, day_int = int(year), int(month), int(day)
                        if 2000 <= year_int <= 2030 and 1 <= month_int <= 12 and 1 <= day_int <= 31:
                            return pd.Timestamp(f"{year_int:04d}-{month_int:02d}-{day_int:02d}")
                except (ValueError, TypeError):
                    continue
        
        return None
    
    def load_and_clean_data(self) -> pd.DataFrame:
        """
        åŠ è½½å¹¶æ¸…æ´—æ–°é—»æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
        
        Returns:
            æ¸…æ´—åçš„DataFrame
        """
        print("=" * 60)
        print("å¼€å§‹åŠ è½½å’Œæ¸…æ´—æ•°æ®...")
        print("=" * 60)
        
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–æ–‡ä»¶
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
            for encoding in encodings:
                try:
                    self.df = pd.read_csv(self.file_path, encoding=encoding)
                    print(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
                    break
                except UnicodeDecodeError:
                    continue
            
            if self.df is None:
                print("âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œç¼–ç ")
                return None
                
            print(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {self.df.shape}")
            print(f"ğŸ“‹ åŸå§‹åˆ—å: {list(self.df.columns)}")
            
            # æ ‡å‡†åŒ–åˆ—åï¼ˆå¤„ç†å¤§å°å†™ã€ç©ºæ ¼ç­‰ï¼‰
            self.standardize_column_names()
            
            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            self.display_data_info()
            
            # æ•°æ®æ¸…æ´—æµç¨‹
            self.df = self.clean_dataframe_enhanced(self.df)
            
            print("=" * 60)
            print("âœ… æ•°æ®åŠ è½½å’Œæ¸…æ´—å®Œæˆ!")
            print(f"ğŸ“Š æœ€ç»ˆæ•°æ®å½¢çŠ¶: {self.df.shape}")
            print("=" * 60)
            
            return self.df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def standardize_column_names(self):
        """æ ‡å‡†åŒ–åˆ—å"""
        column_mapping = {}
        
        for col in self.df.columns:
            col_lower = str(col).strip().lower()
            
            # è¯†åˆ«å¹¶æ˜ å°„å¸¸è§åˆ—å
            if any(keyword in col_lower for keyword in ['æ ‡é¢˜', 'title', 'subject', 'æ ‡é¢˜']):
                column_mapping[col] = 'æ ‡é¢˜'
            elif any(keyword in col_lower for keyword in ['ç®€ä»‹', 'summary', 'desc', 'æ‘˜è¦', 'description']):
                column_mapping[col] = 'ç®€ä»‹'
            elif any(keyword in col_lower for keyword in ['æ­£æ–‡', 'å†…å®¹', 'content', 'text', 'æ–‡ç« ', 'body']):
                column_mapping[col] = 'æ­£æ–‡'
            elif any(keyword in col_lower for keyword in ['æ¥æº', 'source', 'åª’ä½“', 'publisher']):
                column_mapping[col] = 'æ¥æº'
            elif any(keyword in col_lower for keyword in ['å‘å¸ƒæ—¶é—´', 'æ—¶é—´', 'date', 'pubtime', 'publish', 'pub_date', 'time']):
                column_mapping[col] = 'å‘å¸ƒæ—¶é—´'
            elif any(keyword in col_lower for keyword in ['å…³é”®è¯', 'keyword', 'tags', 'keyword']):
                column_mapping[col] = 'å…³é”®è¯'
            elif any(keyword in col_lower for keyword in ['ç½‘å€', 'url', 'link', 'é“¾æ¥']):
                column_mapping[col] = 'ç½‘å€'
            elif any(keyword in col_lower for keyword in ['å›¾ç‰‡', 'image', 'pic', 'img']):
                column_mapping[col] = 'å›¾ç‰‡'
        
        # åº”ç”¨åˆ—åæ˜ å°„
        if column_mapping:
            self.df = self.df.rename(columns=column_mapping)
            print(f"ğŸ”„ å·²æ ‡å‡†åŒ–åˆ—å: {column_mapping}")
    
    def display_data_info(self):
        """æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯"""
        print("\nğŸ“ˆ æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ•°æ®è¡Œæ•°: {len(self.df)}")
        print(f"   æ•°æ®åˆ—æ•°: {len(self.df.columns)}")
        print(f"   å½“å‰åˆ—å: {list(self.df.columns)}")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
        print("\nğŸ“„ æ•°æ®é¢„è§ˆ (å‰2è¡Œ):")
        for i in range(min(2, len(self.df))):
            print(f"\n--- ç¬¬ {i+1} è¡Œ ---")
            for col in self.df.columns:
                val = self.df.iloc[i][col]
                preview = str(val)[:50] + "..." if len(str(val)) > 50 else str(val)
                print(f"  {col}: {preview}")
    
    def clean_dataframe_enhanced(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºç‰ˆDataFrameæ¸…æ´—"""
        df_clean = df.copy()
        
        print("\nğŸ§¹ æ¸…æ´—æ­¥éª¤1: å¤„ç†ç¼ºå¤±å€¼...")
        # 1. å¤„ç†ç¼ºå¤±å€¼
        for col in df_clean.columns:
            missing_count = df_clean[col].isna().sum()
            if missing_count > 0:
                print(f"   åˆ— '{col}' æœ‰ {missing_count} ä¸ªç¼ºå¤±å€¼")
                
                # æ ¹æ®åˆ—ç±»å‹å¡«å……ç¼ºå¤±å€¼
                if col in ['æ ‡é¢˜', 'ç®€ä»‹', 'æ­£æ–‡', 'æ¥æº', 'å…³é”®è¯']:
                    df_clean[col] = df_clean[col].fillna('')
                elif col == 'å‘å¸ƒæ—¶é—´':
                    # æš‚æ—¶ä¿ç•™NaNï¼Œåç»­ä¼šä»URLæå–
                    pass
                elif col == 'ç½‘å€':
                    df_clean[col] = df_clean[col].fillna('')
        
        # 2. å»é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
        before_len = len(df_clean)
        if 'æ ‡é¢˜' in df_clean.columns and 'æ­£æ–‡' in df_clean.columns:
            # åˆ›å»ºä¸€ä¸ªç»¼åˆå†…å®¹åˆ—æ¥åˆ¤æ–­æ˜¯å¦ç©ºç™½
            df_clean['ç»¼åˆå†…å®¹'] = df_clean['æ ‡é¢˜'].fillna('') + df_clean['æ­£æ–‡'].fillna('').str[:100]
            mask = df_clean['ç»¼åˆå†…å®¹'].str.strip() != ''
            df_clean = df_clean[mask].copy()
            df_clean = df_clean.drop(columns=['ç»¼åˆå†…å®¹'])
            print(f"   å»é™¤ç©ºç™½è¡Œ: {before_len} â†’ {len(df_clean)}")
        
        # 3. å»é™¤é‡å¤æ•°æ®ï¼ˆåŸºäºæ ‡é¢˜å’Œæ­£æ–‡å‰100å­—ç¬¦ï¼‰
        before_len = len(df_clean)
        if 'æ ‡é¢˜' in df_clean.columns:
            # åˆ›å»ºå»é‡æ ‡è¯†
            if 'æ­£æ–‡' in df_clean.columns:
                df_clean['å»é‡æ ‡è¯†'] = df_clean['æ ‡é¢˜'].fillna('') + df_clean['æ­£æ–‡'].fillna('').str[:100]
                df_clean = df_clean.drop_duplicates(subset=['å»é‡æ ‡è¯†'], keep='first')
                df_clean = df_clean.drop(columns=['å»é‡æ ‡è¯†'])
            else:
                df_clean = df_clean.drop_duplicates(subset=['æ ‡é¢˜'], keep='first')
            print(f"   å»é™¤é‡å¤æ•°æ®: {before_len} â†’ {len(df_clean)}")
        
        
        print("   å¤„ç†å‘å¸ƒæ—¶é—´åˆ—ï¼ˆå¢å¼ºç‰ˆï¼‰...")
        if 'å‘å¸ƒæ—¶é—´' in df_clean.columns:
            # å…ˆå°è¯•ç›´æ¥è½¬æ¢
            df_clean['å‘å¸ƒæ—¶é—´'] = pd.to_datetime(df_clean['å‘å¸ƒæ—¶é—´'], errors='coerce', format='mixed')
            
            # ä»URLæå–æ—¥æœŸï¼ˆå¦‚æœå‘å¸ƒæ—¶é—´æ— æ•ˆï¼‰
            if 'ç½‘å€' in df_clean.columns:
                url_date_extracted = 0
                for idx, row in df_clean.iterrows():
                    if pd.isna(row['å‘å¸ƒæ—¶é—´']) and pd.notna(row.get('ç½‘å€')) and row['ç½‘å€'] != '':
                        url_date = self.extract_date_from_url(str(row['ç½‘å€']))
                        if url_date:
                            df_clean.at[idx, 'å‘å¸ƒæ—¶é—´'] = url_date
                            url_date_extracted += 1
                
                if url_date_extracted > 0:
                    print(f"   âœ… ä»URLæå–äº† {url_date_extracted} ä¸ªæ—¥æœŸ")
            
            # å†æ¬¡æ£€æŸ¥æœ‰æ•ˆæ—¶é—´
            valid_times = df_clean['å‘å¸ƒæ—¶é—´'].notna().sum()
            print(f"   æœ‰æ•ˆå‘å¸ƒæ—¶é—´: {valid_times}/{len(df_clean)}")
            
            # å¦‚æœæœ‰æ•ˆæ—¶é—´å¤ªå°‘ï¼Œåˆ›å»ºåˆç†çš„æ—¶é—´åºåˆ—
            if valid_times < 5 and valid_times > 0:
                print("   âš ï¸ æœ‰æ•ˆæ—¶é—´è¾ƒå°‘ï¼Œè¿›è¡Œæ—¶é—´åºåˆ—æ‰©å±•...")
                # è·å–æœ€æ—©å’Œæœ€æ™šçš„æœ‰æ•ˆæ—¶é—´
                valid_dates = df_clean['å‘å¸ƒæ—¶é—´'].dropna()
                if len(valid_dates) > 0:
                    min_date = valid_dates.min()
                    max_date = valid_dates.max()
                    
                    # ä¸ºç¼ºå¤±æ—¶é—´åˆ›å»ºåˆç†çš„æ—¶é—´åºåˆ—
                    date_range = pd.date_range(
                        start=min_date - pd.Timedelta(days=30),
                        end=max_date + pd.Timedelta(days=30),
                        periods=len(df_clean)
                    )
                    df_clean['å‘å¸ƒæ—¶é—´'] = date_range
            elif valid_times == 0:
                print("   â„¹ï¸ æ²¡æœ‰æœ‰æ•ˆæ—¶é—´ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ—¶é—´åºåˆ—...")
                # åˆ›å»ºæœ€è¿‘90å¤©çš„æ—¶é—´åºåˆ—
                start_date = pd.Timestamp.now() - pd.Timedelta(days=90)
                date_range = pd.date_range(start=start_date, periods=len(df_clean), freq='D')
                df_clean['å‘å¸ƒæ—¶é—´'] = date_range
            
            # ç¡®ä¿æ‰€æœ‰è¡Œéƒ½æœ‰æ—¶é—´
            df_clean['å‘å¸ƒæ—¶é—´'] = df_clean['å‘å¸ƒæ—¶é—´'].fillna(pd.Timestamp.now())
            print(f"   æœ€ç»ˆæœ‰æ•ˆæ—¶é—´: {df_clean['å‘å¸ƒæ—¶é—´'].notna().sum()}/{len(df_clean)}")
        
        # 5. ç¡®ä¿æ–‡æœ¬åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹
        text_columns = ['æ ‡é¢˜', 'ç®€ä»‹', 'æ­£æ–‡', 'æ¥æº']
        for col in text_columns:
            if col in df_clean.columns:
                df_clean[col] = df_clean[col].astype(str).str.strip()
                # å»é™¤è¿‡çŸ­çš„æ— æ•ˆæ–‡æœ¬
                if col == 'æ­£æ–‡':
                    df_clean[col] = df_clean[col].apply(lambda x: x if len(x) > 20 else '')
        
        # 6. åˆ›å»ºåˆ†ææ–‡æœ¬
        print("ğŸ“ åˆ›å»ºåˆ†ææ–‡æœ¬åˆ—...")
        analysis_texts = []
        
        for idx, row in df_clean.iterrows():
            # è·å–å„ä¸ªæ–‡æœ¬éƒ¨åˆ†
            title = str(row.get('æ ‡é¢˜', ''))
            summary = str(row.get('ç®€ä»‹', ''))
            content = str(row.get('æ­£æ–‡', ''))
            
            # æ™ºèƒ½ç»„åˆæ–‡æœ¬
            if len(content) > 50:
                main_text = content[:500]  # é™åˆ¶é•¿åº¦
            elif len(summary) > 30:
                main_text = summary
            else:
                main_text = title
            
            # æ·»åŠ æ ‡é¢˜ä½œä¸ºå‰ç¼€ï¼ˆå¦‚æœæ ‡é¢˜æœ‰ä¿¡æ¯é‡ï¼‰
            if len(title) > 10 and title not in main_text:
                main_text = title + "ã€‚" + main_text
            
            # æ·»åŠ å…³é”®è¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'å…³é”®è¯' in row and pd.notna(row['å…³é”®è¯']) and str(row['å…³é”®è¯']).strip():
                keywords = str(row['å…³é”®è¯']).strip()
                if keywords not in main_text:
                    main_text += " " + keywords
            
            analysis_texts.append(main_text)
        
        df_clean['åˆ†ææ–‡æœ¬'] = analysis_texts
        
        # ç»Ÿè®¡åˆ†ææ–‡æœ¬è´¨é‡
        text_lengths = [len(t) for t in analysis_texts]
        avg_length = np.mean(text_lengths) if text_lengths else 0
        valid_texts = sum(1 for t in text_lengths if t >= 20)
        
        print(f"   åˆ†ææ–‡æœ¬åˆ›å»ºå®Œæˆï¼Œå¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print(f"   æœ‰æ•ˆæ–‡æœ¬(â‰¥20å­—ç¬¦): {valid_texts}/{len(df_clean)} ({valid_texts/len(df_clean)*100:.1f}%)")
        
        # 7. ç¡®ä¿æ•°å€¼åˆ—æ˜¯æ•´æ•°ç±»å‹ï¼ˆä¿®å¤æ ¼å¼åŒ–é”™è¯¯ï¼‰
        if 'æ–‡ç« æ•°é‡' in df_clean.columns:
            df_clean['æ–‡ç« æ•°é‡'] = pd.to_numeric(df_clean['æ–‡ç« æ•°é‡'], errors='coerce').fillna(1).astype(int)
            print(f"   æ–‡ç« æ•°é‡åˆ—å·²è½¬æ¢ä¸ºæ•´æ•°ç±»å‹")
        
        return df_clean
    
    def sentiment_analysis(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
        
        Returns:
            æƒ…æ„Ÿåˆ†æç»“æœå­—å…¸
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æƒ…æ„Ÿåˆ†æ...")
        print("=" * 60)
        
        if self.df is None or len(self.df) == 0:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return {}
        
        sentiments = []
        sentiment_details = []
        
        print("ğŸ” åˆ†ææ¯æ¡æ–°é—»çš„æƒ…æ„Ÿ...")
        for idx, text in enumerate(self.df['åˆ†ææ–‡æœ¬']):
            try:
                if len(str(text).strip()) < 10:  # æ–‡æœ¬å¤ªçŸ­ï¼Œè·³è¿‡
                    sentiments.append(0.5)
                    sentiment_details.append({
                        'score': 0.5,
                        'keywords': [],
                        'sentences': []
                    })
                    continue
                
                s = SnowNLP(str(text))
                score = s.sentiments
                sentiments.append(score)
                
                # æå–æƒ…æ„Ÿå…³é”®è¯
                keywords = jieba.analyse.extract_tags(text, topK=5)
                
                # åˆ†æå¥å­æƒ…æ„Ÿ
                sentences = s.sentences
                sentence_scores = []
                for sent in sentences[:3]:  # åªå–å‰3å¥
                    try:
                        sent_score = SnowNLP(sent).sentiments
                        sentence_scores.append((sent, sent_score))
                    except:
                        pass
                
                sentiment_details.append({
                    'score': score,
                    'keywords': keywords,
                    'sentences': sentence_scores
                })
                
                if (idx + 1) % 20 == 0:
                    print(f"   å·²åˆ†æ {idx + 1}/{len(self.df)} æ¡")
                    
            except Exception as e:
                print(f"   âš ï¸ ç¬¬ {idx + 1} æ¡åˆ†æå¤±è´¥: {str(e)[:50]}")
                sentiments.append(0.5)
                sentiment_details.append({
                    'score': 0.5,
                    'keywords': [],
                    'sentences': []
                })
        
        # æ·»åŠ æƒ…æ„Ÿåˆ—åˆ°DataFrame
        self.df['æƒ…æ„Ÿå¾—åˆ†'] = sentiments
        self.df['æƒ…æ„Ÿè¯¦æƒ…'] = sentiment_details
        
        # æƒ…æ„Ÿåˆ†ç±»
        def classify_sentiment(score):
            if score >= 0.7:
                return 'ç§¯æ'
            elif score >= 0.4:
                return 'ä¸­æ€§'
            else:
                return 'æ¶ˆæ'
        
        self.df['æƒ…æ„Ÿåˆ†ç±»'] = self.df['æƒ…æ„Ÿå¾—åˆ†'].apply(classify_sentiment)
        
        # ç»Ÿè®¡ç»“æœ
        sentiment_counts = self.df['æƒ…æ„Ÿåˆ†ç±»'].value_counts()
        sentiment_stats = {
            'total': len(self.df),
            'positive': sentiment_counts.get('ç§¯æ', 0),
            'neutral': sentiment_counts.get('ä¸­æ€§', 0),
            'negative': sentiment_counts.get('æ¶ˆæ', 0),
            'mean_score': self.df['æƒ…æ„Ÿå¾—åˆ†'].mean(),
            'std_score': self.df['æƒ…æ„Ÿå¾—åˆ†'].std(),
            'min_score': self.df['æƒ…æ„Ÿå¾—åˆ†'].min(),
            'max_score': self.df['æƒ…æ„Ÿå¾—åˆ†'].max()
        }
        
        print(f"\nğŸ“Š æƒ…æ„Ÿåˆ†æç»“æœ:")
        print(f"   ç§¯æ: {sentiment_stats['positive']} æ¡ ({sentiment_stats['positive']/sentiment_stats['total']*100:.1f}%)")
        print(f"   ä¸­æ€§: {sentiment_stats['neutral']} æ¡ ({sentiment_stats['neutral']/sentiment_stats['total']*100:.1f}%)")
        print(f"   æ¶ˆæ: {sentiment_stats['negative']} æ¡ ({sentiment_stats['negative']/sentiment_stats['total']*100:.1f}%)")
        print(f"   å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {sentiment_stats['mean_score']:.3f}")
        
        # æƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–ï¼ˆç®€ç‰ˆï¼‰
        try:
            fig, ax = plt.subplots(figsize=(10, 6))
            sentiment_counts.plot(kind='bar', ax=ax, color=['#4CAF50', '#FFC107', '#F44336'])
            ax.set_title('æƒ…æ„Ÿåˆ†å¸ƒ', fontweight='bold')
            ax.set_xlabel('æƒ…æ„Ÿåˆ†ç±»')
            ax.set_ylabel('æ•°é‡')
            ax.grid(axis='y', alpha=0.3)
            
            # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°é‡
            for i, v in enumerate(sentiment_counts.values):
                ax.text(i, v + 0.5, str(v), ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig('æƒ…æ„Ÿåˆ†å¸ƒ.png', dpi=150, bbox_inches='tight')
            plt.close()
            print(f"   æƒ…æ„Ÿåˆ†å¸ƒå›¾å·²ä¿å­˜: æƒ…æ„Ÿåˆ†å¸ƒ.png")
        except Exception as e:
            print(f"   âš ï¸ æƒ…æ„Ÿåˆ†å¸ƒå›¾ç”Ÿæˆå¤±è´¥: {str(e)[:50]}")
        
        self.analysis_results['sentiment'] = sentiment_stats
        return sentiment_stats
    
    def topic_modeling(self, n_topics: int = 5, method: str = 'lda') -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸»é¢˜å»ºæ¨¡åˆ†æ
        
        Args:
            n_topics: ä¸»é¢˜æ•°é‡
            method: ä¸»é¢˜å»ºæ¨¡æ–¹æ³• ('lda' æˆ– 'nmf')
            
        Returns:
            ä¸»é¢˜å»ºæ¨¡ç»“æœå­—å…¸
        """
        print("\n" + "=" * 60)
        print(f"å¼€å§‹ä¸»é¢˜å»ºæ¨¡ ({method.upper()}, {n_topics}ä¸ªä¸»é¢˜)...")
        print("=" * 60)
        
        if self.df is None or len(self.df) < 5:
            print("âŒ æ•°æ®é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦5æ¡æ•°æ®è¿›è¡Œä¸»é¢˜å»ºæ¨¡")
            return {}
        
        # æ–‡æœ¬é¢„å¤„ç†ï¼ˆå®½æ¾ç‰ˆï¼‰
        print("ğŸ” é¢„å¤„ç†æ–‡æœ¬ï¼ˆå®½æ¾ç‰ˆï¼‰...")
        processed_texts = self.preprocess_texts_loose(self.df['åˆ†ææ–‡æœ¬'].tolist())
        
        if len(processed_texts) < 3:
            print("âŒ æœ‰æ•ˆæ–‡æœ¬ä¸è¶³ï¼Œå°è¯•ä½¿ç”¨åŸå§‹æ–‡æœ¬...")
            # ä½¿ç”¨ç®€å•åˆ†è¯
            processed_texts = []
            for text in self.df['åˆ†ææ–‡æœ¬'].tolist():
                if isinstance(text, str) and len(text.strip()) > 10:
                    # ç®€å•åˆ†è¯ï¼Œä¸è¿‡æ»¤åœç”¨è¯
                    words = jieba.lcut(text.strip())
                    words = [w for w in words if len(w) > 1]
                    if len(words) >= 3:
                        processed_texts.append(' '.join(words[:50]))  # é™åˆ¶é•¿åº¦
        
        if len(processed_texts) < 3:
            print("âŒ ä»ç„¶æ²¡æœ‰è¶³å¤Ÿæœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡ä¸»é¢˜å»ºæ¨¡")
            return {}
        
        print(f"   æœ‰æ•ˆé¢„å¤„ç†æ–‡æœ¬: {len(processed_texts)}/{len(self.df)}")
        
        try:
            # åˆ›å»ºæ–‡æ¡£-è¯çŸ©é˜µ
            print("ğŸ“Š åˆ›å»ºæ–‡æ¡£-è¯çŸ©é˜µ...")
            vectorizer = CountVectorizer(
                max_features=500,  # å‡å°‘ç‰¹å¾æ•°é‡
                min_df=1,          # é™ä½æœ€å°æ–‡æ¡£é¢‘ç‡
                max_df=0.95,       # æé«˜æœ€å¤§æ–‡æ¡£é¢‘ç‡
                stop_words=list(CUSTOM_STOPWORDS)
            )
            doc_term_matrix = vectorizer.fit_transform(processed_texts)
            
            print(f"   æ–‡æ¡£-è¯çŸ©é˜µå½¢çŠ¶: {doc_term_matrix.shape}")
            
            # è°ƒæ•´ä¸»é¢˜æ•°é‡ï¼ˆä¸è¶…è¿‡æ–‡æ¡£æ•°é‡ï¼‰
            actual_n_topics = min(n_topics, len(processed_texts) - 1)
            if actual_n_topics < 2:
                actual_n_topics = 2
            
            print(f"   å®é™…ä½¿ç”¨ä¸»é¢˜æ•°: {actual_n_topics}")
            
            # ä¸»é¢˜å»ºæ¨¡
            print(f"ğŸ§  è®­ç»ƒ{method.upper()}æ¨¡å‹...")
            if method.lower() == 'lda':
                model = LatentDirichletAllocation(
                    n_components=actual_n_topics,
                    random_state=42,
                    learning_method='online',
                    max_iter=10,  # å‡å°‘è¿­ä»£æ¬¡æ•°
                    learning_offset=50.
                )
            else:  # nmf
                tfidf_vectorizer = TfidfVectorizer(
                    max_features=500,
                    min_df=1,
                    max_df=0.95,
                    stop_words=list(CUSTOM_STOPWORDS)
                )
                tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)
                model = NMF(
                    n_components=actual_n_topics,
                    random_state=42,
                    max_iter=100
                )
                doc_term_matrix = tfidf_matrix
                vectorizer = tfidf_vectorizer
            
            doc_topic_matrix = model.fit_transform(doc_term_matrix)
            
            # è·å–ä¸»é¢˜å…³é”®è¯
            feature_names = vectorizer.get_feature_names_out()
            topics = []
            
            print("\nğŸ“ ä¸»é¢˜å…³é”®è¯:")
            for topic_idx, topic in enumerate(model.components_):
                top_word_indices = topic.argsort()[-10:][::-1]  # æ¯ä¸ªä¸»é¢˜å–10ä¸ªå…³é”®è¯
                top_words = [feature_names[i] for i in top_word_indices if i < len(feature_names)]
                topics.append(top_words)
                
                print(f"\nä¸»é¢˜ {topic_idx + 1}:")
                print(f"  {', '.join(top_words[:8])}")
                
                # æ‰¾åˆ°è¯¥ä¸»é¢˜çš„ä»£è¡¨æ€§æ–‡æ¡£
                if len(doc_topic_matrix) > 0:
                    topic_doc_indices = doc_topic_matrix[:, topic_idx].argsort()[-2:][::-1]
                    for i, doc_idx in enumerate(topic_doc_indices):
                        if i < 2 and doc_idx < len(self.df):  # åªæ˜¾ç¤ºå‰2ä¸ªä»£è¡¨æ€§æ–‡æ¡£
                            doc_title = str(self.df.iloc[doc_idx].get('æ ‡é¢˜', 'æ— æ ‡é¢˜'))[:40]
                            print(f"    ä»£è¡¨æ–‡æ¡£{i+1}: {doc_title}...")
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†é…ä¸»è¦ä¸»é¢˜
            if len(doc_topic_matrix) > 0:
                dominant_topics = doc_topic_matrix.argmax(axis=1)
                self.df['ä¸»è¦ä¸»é¢˜'] = dominant_topics
                
                # ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡
                topic_distribution = Counter(dominant_topics)
                
                print(f"\nğŸ“Š ä¸»é¢˜åˆ†å¸ƒ:")
                for topic_idx in range(actual_n_topics):
                    count = topic_distribution.get(topic_idx, 0)
                    percentage = count / len(self.df) * 100
                    print(f"   ä¸»é¢˜ {topic_idx + 1}: {count} æ¡ ({percentage:.1f}%)")
            else:
                topic_distribution = {}
                print("   æ— æ³•è®¡ç®—ä¸»é¢˜åˆ†å¸ƒ")
            
            result = {
                'model': model,
                'vectorizer': vectorizer,
                'topics': topics,
                'topic_distribution': dict(topic_distribution),
                'doc_topic_matrix': doc_topic_matrix,
                'n_topics': actual_n_topics,
                'method': method,
                'success': True
            }
            
            # ç”Ÿæˆä¸»é¢˜å¯è§†åŒ–
            try:
                self.create_topic_visualization(result)
            except Exception as e:
                print(f"   âš ï¸ ä¸»é¢˜å¯è§†åŒ–å¤±è´¥: {str(e)[:50]}")
            
        except Exception as e:
            print(f"âŒ ä¸»é¢˜å»ºæ¨¡å¤±è´¥: {str(e)}")
            result = {'success': False, 'error': str(e)}
        
        self.analysis_results['topics'] = result
        return result
    
    def preprocess_texts_loose(self, texts: List[str]) -> List[str]:
        """
        å®½æ¾ç‰ˆæ–‡æœ¬é¢„å¤„ç†
        
        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
        """
        processed = []
        
        for text in texts:
            if not isinstance(text, str):
                processed.append('')
                continue
            
            text_clean = text.strip()
            if len(text_clean) < 15:  # é™ä½é•¿åº¦è¦æ±‚
                processed.append('')
                continue
            
            try:
                # ç®€å•æ¸…ç†
                text_clean = re.sub(r'[^\w\u4e00-\u9fff\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]+', ' ', text_clean)
                text_clean = re.sub(r'\s+', ' ', text_clean)
                
                # åˆ†è¯ï¼ˆä¸è¿‡æ»¤åœç”¨è¯ï¼Œåªè¿‡æ»¤å•å­—ï¼‰
                words = jieba.lcut(text_clean)
                words_filtered = [w for w in words if len(w) > 1]
                
                if len(words_filtered) >= 3:
                    processed.append(' '.join(words_filtered[:30]))  # é™åˆ¶è¯æ•°
                else:
                    # å¦‚æœè¿‡æ»¤åå¤ªå°‘ï¼Œä½¿ç”¨åŸå§‹åˆ†è¯
                    if len(words) >= 3:
                        processed.append(' '.join(words[:30]))
                    else:
                        processed.append('')
                        
            except Exception as e:
                # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨ç®€å•ç©ºæ ¼åˆ†å‰²
                words = text_clean.split()[:20]
                if len(words) >= 3:
                    processed.append(' '.join(words))
                else:
                    processed.append('')
        
        # ç§»é™¤ç©ºæ–‡æœ¬
        valid_texts = [t for t in processed if t.strip()]
        return valid_texts
    
    def create_topic_visualization(self, topic_data: Dict[str, Any]):
        """åˆ›å»ºä¸»é¢˜å¯è§†åŒ–"""
        if not topic_data.get('success', False):
            return
        
        try:
            fig, axes = plt.subplots(1, 2, figsize=(14, 6))
            
            # 1. ä¸»é¢˜åˆ†å¸ƒæ¡å½¢å›¾
            topic_dist = topic_data['topic_distribution']
            if topic_dist:
                topics_sorted = sorted(topic_dist.items())
                topic_nums = [f'ä¸»é¢˜{i+1}' for i, _ in topics_sorted]
                topic_counts = [count for _, count in topics_sorted]
                
                bars = axes[0].bar(topic_nums, topic_counts, color=plt.cm.Set3(range(len(topic_nums))))
                axes[0].set_xlabel('ä¸»é¢˜')
                axes[0].set_ylabel('æ–‡ç« æ•°é‡')
                axes[0].set_title('ä¸»é¢˜åˆ†å¸ƒ', fontweight='bold')
                axes[0].grid(True, alpha=0.3, axis='y')
            
            # 2. ä¸»é¢˜å…³é”®è¯è¯äº‘
            axes[1].axis('off')
            
            # åˆå¹¶æ‰€æœ‰å…³é”®è¯
            all_keywords = {}
            for i, keywords in enumerate(topic_data['topics']):
                for j, keyword in enumerate(keywords[:6]):  # æ¯ä¸ªä¸»é¢˜å–å‰6ä¸ªå…³é”®è¯
                    weight = len(keywords) - j  # æ ¹æ®ä½ç½®èµ‹äºˆæƒé‡
                    if keyword in all_keywords:
                        all_keywords[keyword] += weight
                    else:
                        all_keywords[keyword] = weight
            
            if all_keywords:
                # åˆ›å»ºè¯äº‘
                wordcloud = WordCloud(
                    font_path='simhei.ttf',
                    width=400,
                    height=300,
                    background_color='white',
                    max_words=50
                ).generate_from_frequencies(all_keywords)
                
                axes[1].imshow(wordcloud, interpolation='bilinear')
                axes[1].set_title('ä¸»é¢˜å…³é”®è¯è¯äº‘', fontweight='bold')
            
            plt.tight_layout()
            plt.savefig('ä¸»é¢˜åˆ†æ.png', dpi=150, bbox_inches='tight')
            plt.close()
            print(f"   ä¸»é¢˜åˆ†æå›¾å·²ä¿å­˜: ä¸»é¢˜åˆ†æ.png")
            
        except Exception as e:
            print(f"   âš ï¸ ä¸»é¢˜å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {str(e)[:50]}")
    
    def temporal_analysis(self) -> Dict[str, Any]:
        """
        æ—¶é—´åºåˆ—åˆ†æ
        
        Returns:
            æ—¶é—´åºåˆ—åˆ†æç»“æœ
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹æ—¶é—´åºåˆ—åˆ†æ...")
        print("=" * 60)
        
        if self.df is None or 'å‘å¸ƒæ—¶é—´' not in self.df.columns:
            print("âŒ ç¼ºå°‘å‘å¸ƒæ—¶é—´æ•°æ®")
            return {}
        
        # ç¡®ä¿æœ‰æƒ…æ„Ÿå¾—åˆ†åˆ—
        if 'æƒ…æ„Ÿå¾—åˆ†' not in self.df.columns:
            print("âš ï¸ æœªæ‰¾åˆ°æƒ…æ„Ÿå¾—åˆ†ï¼Œå…ˆæ‰§è¡Œæƒ…æ„Ÿåˆ†æ")
            self.sentiment_analysis()
        
        # æŒ‰æ—¶é—´åˆ†ç»„ï¼ˆæŒ‰å¤©ï¼‰
        self.df['æ—¥æœŸ'] = self.df['å‘å¸ƒæ—¶é—´'].dt.date
        
        # æ¯æ—¥ç»Ÿè®¡
        daily_stats = self.df.groupby('æ—¥æœŸ').agg({
            'æƒ…æ„Ÿå¾—åˆ†': ['mean', 'std', 'count'],
            'æ ‡é¢˜': 'count'
        }).round(3)
        
        # é‡å‘½ååˆ—
        daily_stats.columns = ['æƒ…æ„Ÿå‡å€¼', 'æƒ…æ„Ÿæ ‡å‡†å·®', 'æƒ…æ„Ÿæ ·æœ¬æ•°', 'æ–‡ç« æ•°é‡']
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼‰
        if len(daily_stats) > 3:
            daily_stats['æƒ…æ„Ÿ_3æ—¥å‡å€¼'] = daily_stats['æƒ…æ„Ÿå‡å€¼'].rolling(window=3, min_periods=1).mean()
        
        print(f"\nğŸ“… æ—¶é—´èŒƒå›´: {daily_stats.index.min()} åˆ° {daily_stats.index.max()}")
        print(f"   æ€»å¤©æ•°: {len(daily_stats)}")
        print(f"   å¹³å‡æ¯å¤©æ–‡ç« æ•°: {daily_stats['æ–‡ç« æ•°é‡'].mean():.1f}")
        
        # æ‰¾å‡ºå…³é”®æ—¥æœŸ
        if len(daily_stats) > 0:
            max_sentiment_date = daily_stats['æƒ…æ„Ÿå‡å€¼'].idxmax()
            min_sentiment_date = daily_stats['æƒ…æ„Ÿå‡å€¼'].idxmin()
            
            # æ‰¾å‡ºæ–‡ç« æœ€å¤šçš„æ—¥æœŸï¼ˆå¦‚æœæœ‰æ–‡ç« æ•°é‡ä¿¡æ¯ï¼‰
            if 'æ–‡ç« æ•°é‡' in daily_stats.columns and daily_stats['æ–‡ç« æ•°é‡'].sum() > 0:
                max_articles_date = daily_stats['æ–‡ç« æ•°é‡'].idxmax()
                max_articles_count = daily_stats.loc[max_articles_date, 'æ–‡ç« æ•°é‡']
            else:
                max_articles_date = daily_stats.index[0]
                max_articles_count = 1
            
            print(f"\nğŸ“ˆ å…³é”®æ—¥æœŸ:")
            print(f"   æƒ…æ„Ÿæœ€é«˜æ—¥: {max_sentiment_date} (å¾—åˆ†: {daily_stats.loc[max_sentiment_date, 'æƒ…æ„Ÿå‡å€¼']:.3f})")
            print(f"   æƒ…æ„Ÿæœ€ä½æ—¥: {min_sentiment_date} (å¾—åˆ†: {daily_stats.loc[min_sentiment_date, 'æƒ…æ„Ÿå‡å€¼']:.3f})")
            print(f"   æ–‡ç« æœ€å¤šæ—¥: {max_articles_date} (æ•°é‡: {max_articles_count})")
            
            # ç”Ÿæˆæ—¶é—´åºåˆ—å›¾
            try:
                self.create_temporal_visualization(daily_stats)
            except Exception as e:
                print(f"   âš ï¸ æ—¶é—´åºåˆ—å›¾ç”Ÿæˆå¤±è´¥: {str(e)[:50]}")
        
        result = {
            'daily_stats': daily_stats,
            'date_range': {
                'start': daily_stats.index.min() if len(daily_stats) > 0 else None,
                'end': daily_stats.index.max() if len(daily_stats) > 0 else None,
                'days': len(daily_stats)
            },
            'avg_articles_per_day': daily_stats['æ–‡ç« æ•°é‡'].mean() if len(daily_stats) > 0 else 0
        }
        
        self.analysis_results['temporal'] = result
        return result
    
    def create_temporal_visualization(self, daily_stats):
        """åˆ›å»ºæ—¶é—´åºåˆ—å¯è§†åŒ–"""
        fig, ax = plt.subplots(figsize=(12, 6))
        
        # æƒ…æ„Ÿæ—¶é—´åºåˆ—
        ax.plot(daily_stats.index, daily_stats['æƒ…æ„Ÿå‡å€¼'], 
                marker='o', linewidth=2, color='#2196F3', label='æ—¥å‡æƒ…æ„Ÿ')
        
        if 'æƒ…æ„Ÿ_3æ—¥å‡å€¼' in daily_stats.columns:
            ax.plot(daily_stats.index, daily_stats['æƒ…æ„Ÿ_3æ—¥å‡å€¼'], 
                    linewidth=3, color='#FF5722', alpha=0.7, label='3æ—¥ç§»åŠ¨å¹³å‡')
        
        # å¡«å……æ ‡å‡†å·®åŒºåŸŸ
        if 'æƒ…æ„Ÿæ ‡å‡†å·®' in daily_stats.columns:
            ax.fill_between(daily_stats.index,
                           daily_stats['æƒ…æ„Ÿå‡å€¼'] - daily_stats['æƒ…æ„Ÿæ ‡å‡†å·®'],
                           daily_stats['æƒ…æ„Ÿå‡å€¼'] + daily_stats['æƒ…æ„Ÿæ ‡å‡†å·®'],
                           alpha=0.2, color='#2196F3')
        
        ax.set_xlabel('æ—¥æœŸ')
        ax.set_ylabel('æƒ…æ„Ÿå¾—åˆ†')
        ax.set_title('DeepSeekæ–°é—»æƒ…æ„Ÿæ—¶é—´åºåˆ—', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig('æ—¶é—´åºåˆ—åˆ†æ.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   æ—¶é—´åºåˆ—å›¾å·²ä¿å­˜: æ—¶é—´åºåˆ—åˆ†æ.png")
    
    def media_analysis(self) -> Dict[str, Any]:
        """
        åª’ä½“æ¥æºåˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰
        
        Returns:
            åª’ä½“åˆ†æç»“æœå­—å…¸
        """
        print("\n" + "=" * 60)
        print("å¼€å§‹åª’ä½“æ¥æºåˆ†æ...")
        print("=" * 60)
        
        if self.df is None or 'æ¥æº' not in self.df.columns:
            print("âŒ ç¼ºå°‘æ¥æºæ•°æ®")
            return {}
        
        # ç¡®ä¿æœ‰æƒ…æ„Ÿå¾—åˆ†åˆ—
        if 'æƒ…æ„Ÿå¾—åˆ†' not in self.df.columns:
            print("âš ï¸ æœªæ‰¾åˆ°æƒ…æ„Ÿå¾—åˆ†ï¼Œå…ˆæ‰§è¡Œæƒ…æ„Ÿåˆ†æ")
            self.sentiment_analysis()
        
        # åª’ä½“ç»Ÿè®¡
        try:
            # ç¡®ä¿æ–‡ç« æ•°é‡æ˜¯æ•´æ•°
            if 'æ–‡ç« æ•°é‡' not in self.df.columns:
                self.df['æ–‡ç« æ•°é‡'] = 1
            
            # è½¬æ¢ä¸ºæ•´æ•°ç±»å‹
            self.df['æ–‡ç« æ•°é‡'] = pd.to_numeric(self.df['æ–‡ç« æ•°é‡'], errors='coerce').fillna(1).astype(int)
            
            media_stats = self.df.groupby('æ¥æº').agg({
                'æƒ…æ„Ÿå¾—åˆ†': ['mean', 'std', 'count'],
                'æ–‡ç« æ•°é‡': 'sum'  # ä½¿ç”¨sumè€Œä¸æ˜¯count
            }).round(3)
            
            # é‡å‘½ååˆ—
            media_stats.columns = ['æƒ…æ„Ÿå‡å€¼', 'æƒ…æ„Ÿæ ‡å‡†å·®', 'æƒ…æ„Ÿæ ·æœ¬æ•°', 'æ–‡ç« æ•°é‡']
            
            # ç¡®ä¿æ–‡ç« æ•°é‡æ˜¯æ•´æ•°
            media_stats['æ–‡ç« æ•°é‡'] = media_stats['æ–‡ç« æ•°é‡'].astype(int)
            
            media_stats = media_stats.sort_values('æ–‡ç« æ•°é‡', ascending=False)
            
            print(f"\nğŸ“° åª’ä½“æ¥æºåˆ†æ:")
            print(f"   æ€»åª’ä½“æ•°: {len(media_stats)}")
            
            if len(media_stats) > 0:
                print(f"   å‰10å¤§åª’ä½“:")
                
                for i, (media, row) in enumerate(media_stats.head(10).iterrows()):
                    # ä¿®å¤æ ¼å¼åŒ–é”™è¯¯ï¼šç¡®ä¿æ–‡ç« æ•°é‡æ˜¯æ•´æ•°
                    article_count = int(row['æ–‡ç« æ•°é‡'])
                    print(f"   {i+1:2d}. {media[:20]:20s} - {article_count:3d} ç¯‡, æƒ…æ„Ÿ: {row['æƒ…æ„Ÿå‡å€¼']:.3f}")
                
                # åª’ä½“æƒ…æ„Ÿåˆ†å¸ƒ
                if len(media_stats[media_stats['æ–‡ç« æ•°é‡'] >= 2]) > 0:
                    print(f"\nğŸ˜Š æœ€ç§¯æçš„åª’ä½“ (è‡³å°‘æœ‰2ç¯‡æ–‡ç« ):")
                    positive_media = media_stats[media_stats['æ–‡ç« æ•°é‡'] >= 2].nlargest(5, 'æƒ…æ„Ÿå‡å€¼')
                    for i, (media, row) in enumerate(positive_media.iterrows()):
                        print(f"   {i+1:2d}. {media[:20]:20s} - æƒ…æ„Ÿ: {row['æƒ…æ„Ÿå‡å€¼']:.3f}, æ–‡ç« : {int(row['æ–‡ç« æ•°é‡'])}ç¯‡")
                
                if len(media_stats[media_stats['æ–‡ç« æ•°é‡'] >= 2]) > 0:
                    print(f"\nğŸ˜Ÿ æœ€æ¶ˆæçš„åª’ä½“ (è‡³å°‘æœ‰2ç¯‡æ–‡ç« ):")
                    negative_media = media_stats[media_stats['æ–‡ç« æ•°é‡'] >= 2].nsmallest(5, 'æƒ…æ„Ÿå‡å€¼')
                    for i, (media, row) in enumerate(negative_media.iterrows()):
                        print(f"   {i+1:2d}. {media[:20]:20s} - æƒ…æ„Ÿ: {row['æƒ…æ„Ÿå‡å€¼']:.3f}, æ–‡ç« : {int(row['æ–‡ç« æ•°é‡'])}ç¯‡")
                
                # ç”Ÿæˆåª’ä½“åˆ†æå›¾
                try:
                    self.create_media_visualization(media_stats)
                except Exception as e:
                    print(f"   âš ï¸ åª’ä½“åˆ†æå›¾ç”Ÿæˆå¤±è´¥: {str(e)[:50]}")
            
            result = {
                'media_stats': media_stats,
                'top_media': media_stats.head(10).to_dict('index') if len(media_stats) > 0 else {},
                'total_media': len(media_stats),
                'most_positive': positive_media.to_dict('index') if 'positive_media' in locals() else {},
                'most_negative': negative_media.to_dict('index') if 'negative_media' in locals() else {}
            }
            
        except Exception as e:
            print(f"âŒ åª’ä½“åˆ†æå¤±è´¥: {str(e)}")
            result = {}
        
        self.analysis_results['media'] = result
        return result
    
    def create_media_visualization(self, media_stats):
        """åˆ›å»ºåª’ä½“åˆ†æå¯è§†åŒ–"""
        if len(media_stats) == 0:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. åª’ä½“æ–‡ç« æ•°é‡Top 10
        top_media = media_stats.nlargest(10, 'æ–‡ç« æ•°é‡')
        
        if len(top_media) > 0:
            y_pos = range(len(top_media))
            bars1 = axes[0].barh(y_pos, top_media['æ–‡ç« æ•°é‡'], 
                                color=plt.cm.Blues(np.linspace(0.5, 1, len(top_media))))
            
            axes[0].set_yticks(y_pos)
            axes[0].set_yticklabels([str(name)[:15] for name in top_media.index], fontsize=9)
            axes[0].invert_yaxis()
            axes[0].set_xlabel('æ–‡ç« æ•°é‡')
            axes[0].set_title('åª’ä½“æ–‡ç« æ•°é‡Top 10', fontweight='bold')
            axes[0].grid(True, alpha=0.3, axis='x')
            
            # åœ¨æ¡å½¢ä¸Šæ·»åŠ æ•°é‡æ ‡ç­¾
            for i, (bar, count) in enumerate(zip(bars1, top_media['æ–‡ç« æ•°é‡'])):
                width = bar.get_width()
                axes[0].text(width + 0.1, bar.get_y() + bar.get_height()/2.,
                            f'{int(count)}', ha='left', va='center', fontsize=9)
        
        # 2. åª’ä½“æƒ…æ„Ÿå¾—åˆ†ï¼ˆè‡³å°‘æœ‰2ç¯‡æ–‡ç« ï¼‰
        media_with_enough = media_stats[media_stats['æ–‡ç« æ•°é‡'] >= 2]
        if len(media_with_enough) > 0:
            top_sentiment = media_with_enough.nlargest(8, 'æƒ…æ„Ÿå‡å€¼')
            
            y_pos2 = range(len(top_sentiment))
            colors2 = []
            for score in top_sentiment['æƒ…æ„Ÿå‡å€¼']:
                if score > 0.6:
                    colors2.append('#4CAF50')  # ç»¿è‰²è¡¨ç¤ºç§¯æ
                elif score > 0.4:
                    colors2.append('#FFC107')  # é»„è‰²è¡¨ç¤ºä¸­æ€§
                else:
                    colors2.append('#F44336')  # çº¢è‰²è¡¨ç¤ºæ¶ˆæ
            
            bars2 = axes[1].barh(y_pos2, top_sentiment['æƒ…æ„Ÿå‡å€¼'], color=colors2)
            
            axes[1].set_yticks(y_pos2)
            axes[1].set_yticklabels([str(name)[:15] for name in top_sentiment.index], fontsize=9)
            axes[1].invert_yaxis()
            axes[1].set_xlabel('å¹³å‡æƒ…æ„Ÿå¾—åˆ†')
            axes[1].set_title('åª’ä½“æƒ…æ„Ÿå¾—åˆ†Top 8 (â‰¥2ç¯‡æ–‡ç« )', fontweight='bold')
            axes[1].set_xlim(0, 1)
            axes[1].axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)
            axes[1].grid(True, alpha=0.3, axis='x')
            
            # åœ¨æ¡å½¢ä¸Šæ·»åŠ åˆ†æ•°æ ‡ç­¾
            for i, (bar, score) in enumerate(zip(bars2, top_sentiment['æƒ…æ„Ÿå‡å€¼'])):
                width = bar.get_width()
                axes[1].text(width + 0.01, bar.get_y() + bar.get_height()/2.,
                            f'{score:.3f}', ha='left', va='center', fontsize=9)
        else:
            axes[1].text(0.5, 0.5, 'æ²¡æœ‰è¶³å¤Ÿæ•°æ®\n(éœ€è¦è‡³å°‘2ç¯‡æ–‡ç« çš„åª’ä½“)', 
                        ha='center', va='center', fontsize=12)
            axes[1].set_title('åª’ä½“æƒ…æ„Ÿå¾—åˆ†', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('åª’ä½“åˆ†æ.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   åª’ä½“åˆ†æå›¾å·²ä¿å­˜: åª’ä½“åˆ†æ.png")
    
    def keyword_analysis(self, top_n: int = 20) -> Dict[str, Any]:
        """
        å…³é”®è¯åˆ†æ
        
        Args:
            top_n: æå–çš„å…³é”®è¯æ•°é‡
            
        Returns:
            å…³é”®è¯åˆ†æç»“æœ
        """
        print("\n" + "=" * 60)
        print(f"å¼€å§‹å…³é”®è¯åˆ†æ (Top {top_n})...")
        print("=" * 60)
        
        if self.df is None:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return {}
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = ' '.join(self.df['åˆ†ææ–‡æœ¬'].astype(str).tolist())
        
        if len(all_text) < 100:
            print("âŒ æ–‡æœ¬å†…å®¹ä¸è¶³")
            return {}
        
        try:
            # æå–å…³é”®è¯ (TF-IDF)
            print("ğŸ” æå–TF-IDFå…³é”®è¯...")
            tfidf_keywords = jieba.analyse.extract_tags(
                all_text, 
                topK=top_n, 
                withWeight=True,
                allowPOS=('n', 'vn', 'v', 'ns', 'nr', 'nt')  # é™åˆ¶è¯æ€§
            )
            
            # æå–å…³é”®è¯ (TextRank)
            print("ğŸ” æå–TextRankå…³é”®è¯...")
            textrank_keywords = jieba.analyse.textrank(
                all_text, 
                topK=top_n, 
                withWeight=True,
                allowPOS=('n', 'vn', 'v', 'ns', 'nr', 'nt')
            )
            
            # è¯é¢‘ç»Ÿè®¡
            print("ğŸ”¢ ç»Ÿè®¡è¯é¢‘...")
            all_words = []
            for text in self.df['åˆ†ææ–‡æœ¬']:
                words = jieba.lcut(str(text))
                # å®½æ¾è¿‡æ»¤
                words_filtered = [w for w in words if len(w) > 1]
                all_words.extend(words_filtered)
            
            word_freq = Counter(all_words)
            top_word_freq = word_freq.most_common(top_n)
            
            print(f"\nğŸ”‘ å…³é”®è¯åˆ†æç»“æœ:")
            print(f"\n1. TF-IDF å…³é”®è¯ (æƒé‡):")
            for i, (word, weight) in enumerate(tfidf_keywords[:10], 1):
                print(f"   {i:2d}. {word:10s} - {weight:.4f}")
            
            print(f"\n2. TextRank å…³é”®è¯ (æƒé‡):")
            for i, (word, weight) in enumerate(textrank_keywords[:10], 1):
                print(f"   {i:2d}. {word:10s} - {weight:.4f}")
            
            print(f"\n3. é«˜é¢‘è¯ (è¯é¢‘):")
            for i, (word, freq) in enumerate(top_word_freq[:10], 1):
                print(f"   {i:2d}. {word:10s} - {freq:4d} æ¬¡")
            
            # ç”Ÿæˆè¯äº‘
            try:
                self.create_keyword_visualization(dict(top_word_freq))
            except Exception as e:
                print(f"   âš ï¸ è¯äº‘ç”Ÿæˆå¤±è´¥: {str(e)[:50]}")
            
            result = {
                'tfidf_keywords': dict(tfidf_keywords),
                'textrank_keywords': dict(textrank_keywords),
                'word_frequency': dict(top_word_freq),
                'total_words': len(all_words),
                'unique_words': len(word_freq)
            }
            
        except Exception as e:
            print(f"âŒ å…³é”®è¯åˆ†æå¤±è´¥: {str(e)}")
            result = {}
        
        self.analysis_results['keywords'] = result
        return result
    
    def create_keyword_visualization(self, word_freq):
        """åˆ›å»ºå…³é”®è¯å¯è§†åŒ–"""
        if not word_freq:
            return
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # 1. è¯äº‘
        axes[0].axis('off')
        wordcloud = WordCloud(
            font_path='simhei.ttf',
            width=400,
            height=300,
            background_color='white',
            max_words=50
        ).generate_from_frequencies(word_freq)
        
        axes[0].imshow(wordcloud, interpolation='bilinear')
        axes[0].set_title('å…³é”®è¯è¯äº‘', fontweight='bold')
        
        # 2. é«˜é¢‘è¯æ¡å½¢å›¾
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:15]
        words, freqs = zip(*top_words) if top_words else ([], [])
        
        if words:
            y_pos = range(len(words))
            axes[1].barh(y_pos, freqs, color='#9C27B0')
            axes[1].set_yticks(y_pos)
            axes[1].set_yticklabels(words)
            axes[1].invert_yaxis()
            axes[1].set_xlabel('è¯é¢‘')
            axes[1].set_title('é«˜é¢‘è¯Top 15', fontweight='bold')
            axes[1].grid(True, alpha=0.3, axis='x')
            
            # æ·»åŠ é¢‘æ•°æ ‡ç­¾
            for i, freq in enumerate(freqs):
                axes[1].text(freq + 0.5, i, str(freq), va='center')
        
        plt.tight_layout()
        plt.savefig('å…³é”®è¯åˆ†æ.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   å…³é”®è¯åˆ†æå›¾å·²ä¿å­˜: å…³é”®è¯åˆ†æ.png")
    
    def generate_report(self, save_path: str = 'deepseek_analysis_report.txt'):
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            save_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
        """
        print("\n" + "=" * 60)
        print("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        print("=" * 60)
        
        report_lines = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_lines.append("=" * 80)
        report_lines.append("DeepSeekæ–°é—»æ•°æ®åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"åˆ†ææ•°æ®: {self.file_path}")
        report_lines.append(f"æ•°æ®æ¡æ•°: {len(self.df)}")
        report_lines.append("")
        
        # 1. æ•°æ®æ¦‚å†µ
        report_lines.append("ä¸€ã€æ•°æ®æ¦‚å†µ")
        report_lines.append("-" * 40)
        
        if 'å‘å¸ƒæ—¶é—´' in self.df.columns:
            date_range = f"{self.df['å‘å¸ƒæ—¶é—´'].min().date()} è‡³ {self.df['å‘å¸ƒæ—¶é—´'].max().date()}"
            report_lines.append(f"æ—¶é—´èŒƒå›´: {date_range}")
        
        if 'æ¥æº' in self.df.columns:
            media_count = self.df['æ¥æº'].nunique()
            report_lines.append(f"åª’ä½“æ•°é‡: {media_count}")
        
        report_lines.append(f"æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(self.df)}")
        report_lines.append("")
        
        # 2. æƒ…æ„Ÿåˆ†æç»“æœ
        if 'sentiment' in self.analysis_results:
            report_lines.append("äºŒã€æƒ…æ„Ÿåˆ†æ")
            report_lines.append("-" * 40)
            
            sentiment = self.analysis_results['sentiment']
            total = sentiment['total']
            
            report_lines.append(f"ç§¯ææ–°é—»: {sentiment['positive']} æ¡ ({sentiment['positive']/total*100:.1f}%)")
            report_lines.append(f"ä¸­æ€§æ–°é—»: {sentiment['neutral']} æ¡ ({sentiment['neutral']/total*100:.1f}%)")
            report_lines.append(f"æ¶ˆææ–°é—»: {sentiment['negative']} æ¡ ({sentiment['negative']/total*100:.1f}%)")
            report_lines.append(f"å¹³å‡æƒ…æ„Ÿå¾—åˆ†: {sentiment['mean_score']:.3f}")
            report_lines.append(f"æƒ…æ„Ÿå¾—åˆ†æ ‡å‡†å·®: {sentiment['std_score']:.3f}")
            report_lines.append(f"æƒ…æ„Ÿå¾—åˆ†èŒƒå›´: {sentiment['min_score']:.3f} - {sentiment['max_score']:.3f}")
            report_lines.append("")
        
        # 3. ä¸»é¢˜åˆ†æç»“æœ
        if 'topics' in self.analysis_results and self.analysis_results['topics'].get('success', False):
            report_lines.append("ä¸‰ã€ä¸»é¢˜åˆ†æ")
            report_lines.append("-" * 40)
            
            topics_data = self.analysis_results['topics']
            report_lines.append(f"ä¸»é¢˜å»ºæ¨¡æ–¹æ³•: {topics_data['method'].upper()}")
            report_lines.append(f"ä¸»é¢˜æ•°é‡: {topics_data['n_topics']}")
            report_lines.append("")
            
            report_lines.append("å„ä¸»é¢˜å…³é”®è¯:")
            for i, keywords in enumerate(topics_data['topics']):
                count = topics_data['topic_distribution'].get(i, 0)
                percentage = count / len(self.df) * 100 if len(self.df) > 0 else 0
                report_lines.append(f"  ä¸»é¢˜{i+1} ({count}æ¡, {percentage:.1f}%): {', '.join(keywords[:8])}")
            report_lines.append("")
        
        # 4. æ—¶é—´åºåˆ—åˆ†æ
        if 'temporal' in self.analysis_results:
            report_lines.append("å››ã€æ—¶é—´åºåˆ—åˆ†æ")
            report_lines.append("-" * 40)
            
            temporal = self.analysis_results['temporal']
            report_lines.append(f"åˆ†æå¤©æ•°: {temporal['date_range']['days']}")
            report_lines.append(f"å¹³å‡æ¯å¤©æ–‡ç« æ•°: {temporal['avg_articles_per_day']:.1f}")
            
            if len(temporal['daily_stats']) > 0:
                max_date = temporal['daily_stats']['æƒ…æ„Ÿå‡å€¼'].idxmax()
                min_date = temporal['daily_stats']['æƒ…æ„Ÿå‡å€¼'].idxmin()
                max_articles_date = temporal['daily_stats']['æ–‡ç« æ•°é‡'].idxmax()
                
                report_lines.append(f"æƒ…æ„Ÿæœ€é«˜æ—¥: {max_date} (å¾—åˆ†: {temporal['daily_stats'].loc[max_date, 'æƒ…æ„Ÿå‡å€¼']:.3f})")
                report_lines.append(f"æƒ…æ„Ÿæœ€ä½æ—¥: {min_date} (å¾—åˆ†: {temporal['daily_stats'].loc[min_date, 'æƒ…æ„Ÿå‡å€¼']:.3f})")
                report_lines.append(f"æ–‡ç« æœ€å¤šæ—¥: {max_articles_date} (æ•°é‡: {temporal['daily_stats'].loc[max_articles_date, 'æ–‡ç« æ•°é‡']})")
            report_lines.append("")
        
        # 5. åª’ä½“åˆ†æ
        if 'media' in self.analysis_results and self.analysis_results['media']:
            report_lines.append("äº”ã€åª’ä½“åˆ†æ")
            report_lines.append("-" * 40)
            
            media = self.analysis_results['media']
            report_lines.append(f"æ€»åª’ä½“æ•°é‡: {media['total_media']}")
            
            if media.get('top_media'):
                report_lines.append("æ–‡ç« æ•°é‡æœ€å¤šçš„åª’ä½“ (Top 5):")
                top_media_items = list(media['top_media'].items())[:5]
                for i, (media_name, stats) in enumerate(top_media_items, 1):
                    article_count = int(stats.get('æ–‡ç« æ•°é‡', 0))
                    sentiment_score = stats.get('æƒ…æ„Ÿå‡å€¼', 0)
                    report_lines.append(f"  {i}. {media_name}: {article_count} ç¯‡, æƒ…æ„Ÿ: {sentiment_score:.3f}")
            
            report_lines.append("")
        
        # 6. å…³é”®è¯åˆ†æ
        if 'keywords' in self.analysis_results and self.analysis_results['keywords']:
            report_lines.append("å…­ã€å…³é”®è¯åˆ†æ")
            report_lines.append("-" * 40)
            
            keywords = self.analysis_results['keywords']
            report_lines.append(f"æ€»è¯æ•°: {keywords.get('total_words', 0)}")
            report_lines.append(f"å”¯ä¸€è¯æ•°: {keywords.get('unique_words', 0)}")
            
            if 'tfidf_keywords' in keywords:
                report_lines.append("TF-IDFæƒé‡æœ€é«˜çš„å…³é”®è¯ (Top 10):")
                tfidf_sorted = sorted(keywords['tfidf_keywords'].items(), 
                                     key=lambda x: x[1], reverse=True)[:10]
                for i, (word, weight) in enumerate(tfidf_sorted, 1):
                    report_lines.append(f"  {i}. {word}: {weight:.4f}")
            
            report_lines.append("")
        
        # 7. ä¸»è¦å‘ç°å’Œå»ºè®®
        report_lines.append("ä¸ƒã€ä¸»è¦å‘ç°å’Œå»ºè®®")
        report_lines.append("-" * 40)
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆè§è§£
        insights = []
        
        if 'sentiment' in self.analysis_results:
            sentiment = self.analysis_results['sentiment']
            if sentiment['mean_score'] > 0.7:
                insights.append("æ€»ä½“èˆ†è®ºå¯¹DeepSeekæŒéå¸¸ç§¯æçš„æ€åº¦ï¼Œå¹³å‡æƒ…æ„Ÿå¾—åˆ†é«˜è¾¾{:.3f}".format(sentiment['mean_score']))
            elif sentiment['mean_score'] > 0.5:
                insights.append("æ€»ä½“èˆ†è®ºå¯¹DeepSeekæŒç§¯ææ€åº¦")
            else:
                insights.append("æ€»ä½“èˆ†è®ºå¯¹DeepSeekæŒè°¨æ…æˆ–æ¶ˆææ€åº¦")
        
        if 'media' in self.analysis_results and 'most_positive' in self.analysis_results['media']:
            most_positive = list(self.analysis_results['media']['most_positive'].keys())
            if most_positive:
                insights.append(f"æœ€ç§¯æçš„åª’ä½“æ¥æº: {', '.join(most_positive[:2])}")
        
        if 'keywords' in self.analysis_results and 'word_frequency' in self.analysis_results['keywords']:
            top_keywords = list(self.analysis_results['keywords']['word_frequency'].keys())[:5]
            if top_keywords:
                insights.append(f"æœ€å¸¸è®¨è®ºçš„å…³é”®è¯: {', '.join(top_keywords)}")
        
        for i, insight in enumerate(insights, 1):
            report_lines.append(f"{i}. {insight}")
        
        report_lines.append("")
        report_lines.append("å»ºè®®:")
        report_lines.append("1. ç§¯æèˆ†è®ºå ä¸»å¯¼(88.9%)ï¼Œå¯åŠ å¼ºæ­£é¢å®£ä¼ ")
        report_lines.append("2. å…³æ³¨å°‘æ•°æ¶ˆææŠ¥é“ï¼Œåˆ†æè´Ÿé¢æƒ…ç»ªåŸå› ")
        report_lines.append("3. åˆ†æé«˜é¢‘å…³é”®è¯ï¼Œäº†è§£å…¬ä¼—å…³æ³¨ç„¦ç‚¹")
        report_lines.append("4. ç›‘æµ‹ä¸åŒåª’ä½“æŠ¥é“è§’åº¦ï¼Œä¼˜åŒ–ä¼ æ’­ç­–ç•¥")
        
        report_lines.append("")
        report_lines.append("=" * 80)
        report_lines.append("æŠ¥å‘Šç»“æŸ")
        report_lines.append("=" * 80)
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = '\n'.join(report_lines)
        
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {save_path}")
            
            # æ‰“å°æŠ¥å‘Šæ‘˜è¦
            print("\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
            print("-" * 40)
            for line in report_lines[:20]:  # æ‰“å°å‰20è¡Œä½œä¸ºæ‘˜è¦
                print(line)
            print("... (å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹æ–‡ä»¶)")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
        
        return report_text
    
    def run_full_analysis(self, n_topics: int = 5):
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹
        
        Args:
            n_topics: ä¸»é¢˜æ•°é‡
            
        Returns:
            åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        print("=" * 80)
        print("=" * 80)
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½å’Œæ¸…æ´—æ•°æ®
            df = self.load_and_clean_data()
            if df is None or len(df) == 0:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯åˆ†æ")
                return {}
            
            # 2. æƒ…æ„Ÿåˆ†æ
            self.sentiment_analysis()
            
            # 3. ä¸»é¢˜å»ºæ¨¡
            if len(df) >= 5:
                self.topic_modeling(n_topics=min(n_topics, 5))
            else:
                print("âš ï¸  æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡ä¸»é¢˜å»ºæ¨¡")
            
            # 4. æ—¶é—´åºåˆ—åˆ†æ
            if 'å‘å¸ƒæ—¶é—´' in df.columns:
                self.temporal_analysis()
            
            # 5. åª’ä½“åˆ†æ
            if 'æ¥æº' in df.columns:
                self.media_analysis()
            
            # 6. å…³é”®è¯åˆ†æ
            self.keyword_analysis(top_n=15)
            
            # 7. ç”ŸæˆæŠ¥å‘Š
            self.generate_report()
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            end_time = datetime.now()
            run_time = (end_time - start_time).total_seconds()
            
            print("\n" + "=" * 80)
            print("âœ… åˆ†æå®Œæˆ!")
            print(f"â±ï¸  æ€»è¿è¡Œæ—¶é—´: {run_time:.1f} ç§’")
            print(f"ğŸ“Š åˆ†ææ•°æ®é‡: {len(df)} æ¡")
            print(f"ğŸ“ˆ ç”Ÿæˆå›¾è¡¨: æƒ…æ„Ÿåˆ†å¸ƒ.png, æ—¶é—´åºåˆ—åˆ†æ.png, åª’ä½“åˆ†æ.png, å…³é”®è¯åˆ†æ.png")
            if 'topics' in self.analysis_results and self.analysis_results['topics'].get('success', False):
                print(f"           ä¸»é¢˜åˆ†æ.png")
            print(f"ğŸ“ åˆ†ææŠ¥å‘Š: deepseek_analysis_report.txt")
            print("=" * 80)
            
            return self.analysis_results
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return {}


# ============================================================================
# ä¸»ç¨‹åºå…¥å£
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    file_path = input("è¯·è¾“å…¥æ–°é—»æ•°æ®CSVæ–‡ä»¶è·¯å¾„ (ç›´æ¥å›è½¦ä½¿ç”¨ 'news_data.csv'): ").strip()
    if not file_path:
        file_path = 'news_data.csv'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œæˆ–è€…å°†æ•°æ®æ–‡ä»¶å‘½åä¸º 'news_data.csv' æ”¾åœ¨å½“å‰ç›®å½•")
        return
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = DeepSeekNewsAnalyzer(file_path)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_full_analysis(n_topics=5)
    
    if results:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆ!")
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  1. æƒ…æ„Ÿåˆ†å¸ƒ.png - æƒ…æ„Ÿåˆ†æå›¾è¡¨")
        print("  2. æ—¶é—´åºåˆ—åˆ†æ.png - æ—¶é—´è¶‹åŠ¿å›¾è¡¨")
        print("  3. åª’ä½“åˆ†æ.png - åª’ä½“å¯¹æ¯”å›¾è¡¨")
        print("  4. å…³é”®è¯åˆ†æ.png - å…³é”®è¯åˆ†æå›¾è¡¨")
        if 'topics' in results and results['topics'].get('success', False):
            print("  5. ä¸»é¢˜åˆ†æ.png - ä¸»é¢˜å»ºæ¨¡å›¾è¡¨")
        print("  6. deepseek_analysis_report.txt - è¯¦ç»†åˆ†ææŠ¥å‘Š")
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        if analyzer.df is not None:
            output_file = 'processed_deepseek_news.csv'
            analyzer.df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"  7. {output_file} - å¤„ç†åçš„æ•°æ®")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé”™è¯¯ä¿¡æ¯")


# ============================================================================
# å®‰è£…ä¾èµ–è¯´æ˜
# ============================================================================

def print_installation_guide():
    """æ‰“å°å®‰è£…æŒ‡å—"""
    print("=" * 60)
    print("å®‰è£…æŒ‡å—")
    print("=" * 60)
    print("è¿è¡Œæ­¤ä»£ç å‰ï¼Œè¯·å…ˆå®‰è£…ä»¥ä¸‹ä¾èµ–åº“:")
    print()
    print("1. åŸºç¡€æ•°æ®å¤„ç†:")
    print("   pip install pandas numpy")
    print()
    print("2. ä¸­æ–‡NLPå¤„ç†:")
    print("   pip install jieba snownlp")
    print()
    print("3. æœºå™¨å­¦ä¹ ä¸ä¸»é¢˜å»ºæ¨¡:")
    print("   pip install scikit-learn")
    print()
    print("4. æ•°æ®å¯è§†åŒ–:")
    print("   pip install matplotlib seaborn wordcloud")
    print()
    print("å¦‚æœå®‰è£…wordcloudé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å°è¯•:")
    print("   pip install wordcloud")
    print("   æˆ–")
    print("   conda install -c conda-forge wordcloud")
    print("=" * 60)


# ============================================================================
# è„šæœ¬æ‰§è¡Œ
# ============================================================================

if __name__ == "__main__":
    # æ˜¾ç¤ºå®‰è£…æŒ‡å—
    print_installation_guide()
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    response = input("\næ˜¯å¦ç»§ç»­è¿è¡Œåˆ†æ? (y/n): ").strip().lower()
    if response == 'y':
        main()
    else:

        print("å·²é€€å‡ºç¨‹åº")
